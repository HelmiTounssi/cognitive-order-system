"""
Interface avec un vrai LLM (OpenAI)
G√®re l'extraction d'intentions, de param√®tres et la g√©n√©ration d'embeddings
"""

import os
import json
import openai
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv

# Charge les variables d'environnement
load_dotenv()

class LLMInterface:
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo"):
        """
        Initialise l'interface LLM
        
        Args:
            api_key: Cl√© API OpenAI (optionnel, peut √™tre dans .env)
            model: Mod√®le √† utiliser
        """
        self.model = model
        
        # Configuration de l'API
        if api_key:
            self.client = openai.OpenAI(api_key=api_key)
        else:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("Cl√© API OpenAI manquante. D√©finissez OPENAI_API_KEY dans .env ou passez-la en param√®tre.")
            self.client = openai.OpenAI(api_key=api_key)
        
        # Prompts syst√®me pour l'extraction d'intentions et param√®tres
        self.intent_extraction_prompt = """
Tu es un assistant sp√©cialis√© dans l'analyse de requ√™tes utilisateur pour un syst√®me de gestion de commandes.

Analyse la requ√™te utilisateur et extrait l'intention principale et les param√®tres pertinents.

Intentions possibles :
- create_order : Cr√©ation d'une nouvelle commande
- validate_order : Validation d'une commande existante
- recommend_products : Recommandation de produits
- check_status : V√©rification de statut ou historique
- process_payment : Traitement de paiement
- add_client : Ajout d'un nouveau client
- list_clients : Liste de tous les clients
- introspect_ontology : Introspection de la structure de l'ontologie
- extend_ontology : Extension dynamique de l'ontologie avec une nouvelle classe
- create_instance : Cr√©ation dynamique d'une instance d'une classe
- query_ontology : Requ√™te introspective sur l'ontologie

Param√®tres √† extraire selon l'intention :
- create_order : client_name, products (liste avec product_name et quantity), immediate_payment
- validate_order : order_id
- recommend_products : query_text, reference_product
- check_status : order_id, client_name
- process_payment : order_id, amount
- add_client : client_name, email
- list_clients : (aucun param√®tre)
- introspect_ontology : (aucun param√®tre)
- extend_ontology : class_name, properties (liste avec name, type, label), namespace
- create_instance : class_name, properties (dictionnaire), instance_id
- query_ontology : query_type (classes, properties, instances, structure), class_name

R√©ponds uniquement avec un JSON valide au format :
{
    "intent": "intention_d√©tect√©e",
    "confidence": 0.95,
    "parameters": {
        "param1": "valeur1",
        "param2": "valeur2"
    }
}
"""

        self.embedding_model = "text-embedding-3-small"
    
    def extract_intent_and_parameters(self, user_query: str) -> Tuple[str, Dict, float]:
        """
        Extrait l'intention et les param√®tres d'une requ√™te utilisateur
        
        Args:
            user_query: Requ√™te utilisateur en langage naturel
        
        Returns:
            Tuple[str, Dict, float]: (intention, param√®tres, confiance)
        """
        try:
            # Construction du prompt
            messages = [
                {"role": "system", "content": self.intent_extraction_prompt},
                {"role": "user", "content": f"Requ√™te utilisateur : {user_query}"}
            ]
            
            # Appel √† l'API avec la nouvelle syntaxe
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.1,  # Faible temp√©rature pour plus de coh√©rence
                max_tokens=500
            )
            
            # Parsing de la r√©ponse JSON
            content = response.choices[0].message.content.strip()
            
            # Nettoyage du JSON (suppression de markdown si pr√©sent)
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            
            result = json.loads(content)
            
            intent = result.get("intent", "unknown")
            parameters = result.get("parameters", {})
            confidence = result.get("confidence", 0.0)
            
            print(f"ü§ñ LLM - Intention extraite: {intent} (confiance: {confidence:.2f})")
            print(f"ü§ñ LLM - Param√®tres: {parameters}")
            
            return intent, parameters, confidence
            
        except json.JSONDecodeError as e:
            print(f"‚ùå Erreur de parsing JSON: {e}")
            print(f"Contenu re√ßu: {content}")
            return "unknown", {}, 0.0
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction d'intention: {e}")
            return "unknown", {}, 0.0
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        G√©n√®re un embedding r√©el pour un texte donn√©
        
        Args:
            text: Texte √† encoder
        
        Returns:
            List[float]: Vecteur d'embedding
        """
        try:
            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            
            embedding = response.data[0].embedding
            print(f"ü§ñ LLM - Embedding g√©n√©r√© pour: '{text[:50]}...'")
            
            return embedding
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration d'embedding: {e}")
            # Fallback vers un embedding simul√©
            return self._generate_fallback_embedding(text)
    
    def _generate_fallback_embedding(self, text: str) -> List[float]:
        """
        G√©n√®re un embedding de fallback (simul√©) en cas d'erreur
        
        Args:
            text: Texte √† encoder
        
        Returns:
            List[float]: Vecteur d'embedding simul√©
        """
        import hashlib
        
        # Utilise un hash du texte pour g√©n√©rer un vecteur d√©terministe
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        
        # Convertit les bytes en liste de floats entre -1 et 1
        embedding = []
        for i in range(0, len(hash_bytes), 4):
            chunk = hash_bytes[i:i+4]
            while len(chunk) < 4:
                chunk += b'\x00'
            
            value = int.from_bytes(chunk, byteorder='big')
            normalized_value = (value / (2**32 - 1)) * 2 - 1
            embedding.append(normalized_value)
        
        # S'assure d'avoir une dimension fixe (1536 pour text-embedding-3-small)
        while len(embedding) < 1536:
            remaining = 1536 - len(embedding)
            embedding.extend(embedding[:min(remaining, len(embedding))])
        
        return embedding[:1536]
    
    def get_recommendation_prompt(self, query_text: str, available_products: List[Dict]) -> str:
        """
        G√©n√®re un prompt pour les recommandations de produits
        
        Args:
            query_text: Description du besoin utilisateur
            available_products: Liste des produits disponibles
        
        Returns:
            str: Prompt pour le LLM
        """
        products_info = []
        for product in available_products:
            products_info.append(f"- {product['name']}: {product['description']} ({product['price']}‚Ç¨)")
        
        prompt = f"""
Tu es un assistant sp√©cialis√© dans les recommandations de produits informatiques.

L'utilisateur recherche : "{query_text}"

Produits disponibles :
{chr(10).join(products_info)}

Recommandes 3 produits maximum qui correspondent le mieux au besoin de l'utilisateur.
Pour chaque recommandation, explique bri√®vement pourquoi ce produit est pertinent.

R√©ponds au format :
1. **Nom du produit** - Prix‚Ç¨
   Raison de la recommandation

2. **Nom du produit** - Prix‚Ç¨
   Raison de la recommandation

3. **Nom du produit** - Prix‚Ç¨
   Raison de la recommandation
"""
        return prompt
    
    def get_product_recommendations(self, query_text: str, available_products: List[Dict]) -> str:
        """
        G√©n√®re des recommandations de produits via le LLM
        
        Args:
            query_text: Description du besoin utilisateur
            available_products: Liste des produits disponibles
        
        Returns:
            str: Recommandations format√©es
        """
        try:
            prompt = self.get_recommendation_prompt(query_text, available_products)
            
            messages = [
                {"role": "system", "content": "Tu es un expert en produits informatiques."},
                {"role": "user", "content": prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=800
            )
            
            recommendations = response.choices[0].message.content.strip()
            print(f"ü§ñ LLM - Recommandations g√©n√©r√©es pour: '{query_text}'")
            
            return recommendations
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de recommandations: {e}")
            return "‚ùå Impossible de g√©n√©rer des recommandations pour le moment."
    
    def get_error_explanation(self, error_type: str, context: str) -> str:
        """
        G√©n√®re une explication d'erreur en langage naturel
        
        Args:
            error_type: Type d'erreur
            context: Contexte de l'erreur
        
        Returns:
            str: Explication de l'erreur
        """
        try:
            prompt = f"""
Tu es un assistant sp√©cialis√© dans l'explication d'erreurs techniques.

Type d'erreur: {error_type}
Contexte: {context}

Explique cette erreur de mani√®re claire et propose des solutions possibles.
R√©ponds en fran√ßais de mani√®re concise et utile.
"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Tu es un assistant technique utile."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration d'explication: {e}"
    
    def generate_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """
        G√©n√®re une r√©ponse textuelle avec l'LLM
        
        Args:
            prompt: Prompt √† envoyer √† l'LLM
            temperature: Contr√¥le la cr√©ativit√© (0.0 = d√©terministe, 1.0 = tr√®s cr√©atif)
            max_tokens: Nombre maximum de tokens dans la r√©ponse
        
        Returns:
            str: R√©ponse g√©n√©r√©e par l'LLM
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Tu es un assistant m√©tier intelligent et utile."},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de r√©ponse: {e}")
            return f"D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse. Erreur: {e}" 